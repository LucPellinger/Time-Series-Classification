{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2579479e",
   "metadata": {},
   "source": [
    "# Experiment: Comparison of TimeMIL and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91879df7",
   "metadata": {},
   "source": [
    "##### Setup and Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bcc6e",
   "metadata": {},
   "source": [
    "##### Installing Optuna extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f82d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U optuna\n",
    "#!pip install optuna-dashboard\n",
    "#!pip install optuna-integration\n",
    "# !pip install optuna-integration[pytorch_lightning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc93461",
   "metadata": {},
   "source": [
    "##### Installing dependencies for Interpretability assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd92079",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install captum\n",
    "#!pip install matplotlib\n",
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4dd04",
   "metadata": {},
   "source": [
    "##### Ensuring correct versions installed to use timemil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f392ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Device set to CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Device set to CPU.\")\n",
    "\n",
    "# Check if dependencies were correctly installed\n",
    "\n",
    "import importlib\n",
    "\n",
    "def check_dependency(package_name, expected_version):\n",
    "    try:\n",
    "        package = importlib.import_module(package_name)\n",
    "        installed_version = package.__version__\n",
    "        if installed_version == expected_version:\n",
    "            print(f\"{package_name} is correctly installed: {installed_version}\")\n",
    "        else:\n",
    "            print(f\"{package_name} version mismatch: expected {expected_version}, but found {installed_version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed.\")\n",
    "\n",
    "# Expected versions\n",
    "dependencies = {\n",
    "    \"aeon\": \"0.5.0\",\n",
    "    \"numpy\": \"1.23.1\",\n",
    "    \"torch\": \"1.13.1+cu117\",\n",
    "    \"torchvision\": \"0.14.1+cu117\",\n",
    "    \"pytorch_lightning\": \"1.8.6\",\n",
    "    \"sklearn\": \"1.2.2\",   # scikit-learn is accessed as \"sklearn\"\n",
    "    \"pandas\": \"2.0.3\",\n",
    "    \"joblib\": \"1.4.2\",\n",
    "    \"torchmetrics\": \"1.5.1\"\n",
    "}\n",
    "\n",
    "# Run checks\n",
    "for package, expected_version in dependencies.items():\n",
    "    check_dependency(package, expected_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f078eb86",
   "metadata": {},
   "source": [
    "##### Reloading and chaning system path to newest project base folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b20473",
   "metadata": {},
   "source": [
    "## Running Experiments with Aeon Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_assessment')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'timemil'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name [BasicMotions, SharePriceIncrease]\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset=dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=128,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.2,\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc01e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_assessment')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run TodyNet on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'todynet'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    # Define todynet parameters (without 'num_nodes')\n",
    "    todynet_params = {\n",
    "        'num_layers': 3,\n",
    "        'groups': 4,\n",
    "        'pool_ratio': 0.2,\n",
    "        'kern_size': [9, 5, 3],\n",
    "        'hidden_dim': 128,\n",
    "        'out_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'gnn_model_type': 'dyGIN2d',\n",
    "        # 'in_dim' and 'seq_length' will be set in train_experiment\n",
    "    }\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset=dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=128,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.2,\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "        todynet_params=todynet_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run LSTMClassifier on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'lstm_classifier'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset=dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,         # Increased batch size for faster training if GPU memory allows\n",
    "        hidden_dim=256,        # Increased hidden dimension\n",
    "        num_layers=3,          # Increased number of layers\n",
    "        bidirectional=False,    # Using a bidirectional LSTM\n",
    "        max_seq_len=60,\n",
    "        dropout=0.3,           # Increased dropout to prevent overfitting\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,     # Adjusted weight decay\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=1.0, # Increased gradient clipping value\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335380fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "from train import train_experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c142726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mount Google Drive (if using Google Drive for dataset/code)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies\n",
    "# !pip install torch pytorch-lightning scikit-learn pandas joblib\n",
    "\n",
    "!pip install aeon==0.5.0 numpy==1.23.1 torch==1.13.1+cu117 torchvision==0.14.1+cu117 pytorch-lightning==1.8.6 torchmetrics==1.5.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install scikit-learn pandas joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01a23",
   "metadata": {},
   "source": [
    "## Custom Dataset Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed501e13",
   "metadata": {},
   "source": [
    "##### Running Optimizations for IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "# from train_to_optimize import train_experiment_optimization\n",
    "from train_optimization_v2 import train_experiment_optimization\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "model_name = 'timemil' # replace with \"lstm_classifier\" or \"todynet\"\n",
    "dataset_name = 'imdb'  # Replace with actual dataset name [rotten, twitter]\n",
    "seeds = [46]\n",
    "number_trials = 25\n",
    "\n",
    "for seed in seeds:\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"optuna_{model_name}_{dataset_name}_trials_{number_trials}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment_optimization(\n",
    "        model_name=model_name,\n",
    "        data_dir=\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/imdb\",\n",
    "        experiment_name=experiment_name,\n",
    "        # seed=seed,\n",
    "        max_epochs=200,\n",
    "        max_trials=number_trials,  # Adjust the number of trials as needed\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f47f07",
   "metadata": {},
   "source": [
    "##### TimeMIL (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'timemil'\n",
    "dataset_name = 'imdb'  # Replace with actual dataset name [BasicMotions, SharePriceIncrease]\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        data_dir=\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/imdb\",\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=32, # 128\n",
    "        hidden_dim=128, # never change this for timemil\n",
    "        max_seq_len=70,\n",
    "        dropout=0.4, # current best at 0.4\n",
    "        optimizer='adamw',\n",
    "        lr=1e-5, # current best at 1e-5\n",
    "        weight_decay=1e-4, # current best at 1e-4\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e519c41",
   "metadata": {},
   "source": [
    "##### TodyNet (runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792577a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TodyNet on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'todynet'\n",
    "dataset_name = 'twitter'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    # Define todynet parameters (without 'num_nodes')\n",
    "    todynet_params = {\n",
    "        'num_layers': 3,\n",
    "        'groups': 1, # 1\n",
    "        'pool_ratio': 0.2, #0.2\n",
    "        'kern_size': [9, 5, 3], # [9, 5, 3]\n",
    "        'hidden_dim': 64, # 128\n",
    "        'out_dim': 128, # 256\n",
    "        'dropout': 0.3,\n",
    "        'gnn_model_type': 'dyGCN2d',\n",
    "        'in_dim': 1\n",
    "        # 'in_dim' and 'seq_length' will be set in train_experiment\n",
    "    }\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        data_dir=\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/twitter\",\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=128,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.3,\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "        todynet_params=todynet_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e501908",
   "metadata": {},
   "source": [
    "##### LSTM /tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f68d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run LSTMClassifier on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'lstm_classifier'\n",
    "dataset_name = 'imdb'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        data_dir=\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/imdb\",\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,         # Increased batch size for faster training if GPU memory allows\n",
    "        hidden_dim=256,        # Increased hidden dimension\n",
    "        num_layers=3,          # Increased number of layers\n",
    "        bidirectional=False,    # Using a bidirectional LSTM\n",
    "        max_seq_len=60,\n",
    "        dropout=0.4,           # Increased dropout to prevent overfitting\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,     # Adjusted weight decay\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=1.0, # Increased gradient clipping value\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
