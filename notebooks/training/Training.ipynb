{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a148436",
   "metadata": {},
   "source": [
    "# Experiment: Comparison of TimeMIL and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484d5b7",
   "metadata": {},
   "source": [
    "##### Setup and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a09b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Mount Google Drive (if using Google Drive for dataset/code)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ace83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install dependencies\n",
    "# !pip install torch pytorch-lightning scikit-learn pandas joblib\n",
    "\n",
    "!pip install aeon==0.5.0 numpy==1.23.1 torch==1.13.1+cu117 torchvision==0.14.1+cu117 pytorch-lightning==1.8.6 torchmetrics==1.5.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install scikit-learn pandas joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af012f",
   "metadata": {},
   "source": [
    "##### Installing dependencies for Interpretability assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install captum\n",
    "#!pip install matplotlib\n",
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf707b18",
   "metadata": {},
   "source": [
    "##### Ensuring correct versions installed to use timemil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949d861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Device set to CUDA.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Device set to CPU.\")\n",
    "\n",
    "# Check if dependencies were correctly installed\n",
    "\n",
    "import importlib\n",
    "\n",
    "def check_dependency(package_name, expected_version):\n",
    "    try:\n",
    "        package = importlib.import_module(package_name)\n",
    "        installed_version = package.__version__\n",
    "        if installed_version == expected_version:\n",
    "            print(f\"{package_name} is correctly installed: {installed_version}\")\n",
    "        else:\n",
    "            print(f\"{package_name} version mismatch: expected {expected_version}, but found {installed_version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name} is not installed.\")\n",
    "\n",
    "# Expected versions\n",
    "dependencies = {\n",
    "    \"aeon\": \"0.5.0\",\n",
    "    \"numpy\": \"1.23.1\",\n",
    "    \"torch\": \"1.13.1+cu117\",\n",
    "    \"torchvision\": \"0.14.1+cu117\",\n",
    "    \"pytorch_lightning\": \"1.8.6\",\n",
    "    \"sklearn\": \"1.2.2\",   # scikit-learn is accessed as \"sklearn\"\n",
    "    \"pandas\": \"2.0.3\",\n",
    "    \"joblib\": \"1.4.2\",\n",
    "    \"torchmetrics\": \"1.5.1\"\n",
    "}\n",
    "\n",
    "# Run checks\n",
    "for package, expected_version in dependencies.items():\n",
    "    check_dependency(package, expected_version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55777821",
   "metadata": {},
   "source": [
    "##### Reloading and chaning system path to newest project base folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb115275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "from train import train_experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba876676",
   "metadata": {},
   "source": [
    "## Custom Dataset Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69795d20",
   "metadata": {},
   "source": [
    "# Rotten Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5feec41",
   "metadata": {},
   "source": [
    "##### TimeMIL Rotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e06788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'timemil'\n",
    "dataset_name = 'rotten'  # Replace with actual dataset name [BasicMotions, SharePriceIncrease]\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(0, 30):\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/rotten\",\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=64, # 128\n",
    "        hidden_dim=128, # never change this for timemil\n",
    "        num_layers=4,\n",
    "        max_seq_len=60,\n",
    "        dropout= 0.4, # current best at 0.4\n",
    "        optimizer='adamw',\n",
    "        lr= 1.0309706745004951e-05,# 1e-5, # current best at 1e-5\n",
    "        weight_decay= 1.0859867475239952e-05,# 1e-4, # current best at 1e-4\n",
    "        max_epochs=200,\n",
    "        gradient_clip_val=0.43956229955444975,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f146c8",
   "metadata": {},
   "source": [
    "##### TodyNet Rotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37365d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TodyNet on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'todynet'\n",
    "dataset_name = 'rotten'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(0, 30):\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}_check\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    # Define todynet parameters (without 'num_nodes')\n",
    "    todynet_params = {\n",
    "        'num_layers': 3,\n",
    "        'groups': 1, # 1\n",
    "        'pool_ratio': 0.3728438352957143, #0.2\n",
    "        'kern_size': [9, 5, 3], # [9, 5, 3]\n",
    "        'hidden_dim': 64, # 128\n",
    "        'out_dim': 128, # 256\n",
    "        'dropout': 0.3326848732885655 ,\n",
    "        'gnn_model_type': 'dyGCN2d',\n",
    "        'in_dim': 1\n",
    "        # 'in_dim' and 'seq_length' will be set in train_experiment\n",
    "    }\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/rotten\",\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=192,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.17252497761075708,\n",
    "        optimizer='adamw',\n",
    "        lr=0.00018641731982861303,\n",
    "        weight_decay=2.0321835493435088e-05,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "        todynet_params=todynet_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444a78c",
   "metadata": {},
   "source": [
    "##### Rotten LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d1d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run LSTMClassifier on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'lstm_classifier'\n",
    "dataset_name = 'rotten'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(0, 30):\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}_check\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        model_name=model_name,\n",
    "        dataset_name=dataset_name,\n",
    "        data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/rotten\",\n",
    "        #aeon_dataset='SharePriceIncrease',\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=64,         # Increased batch size for faster training if GPU memory allows\n",
    "        hidden_dim=128,        # Increased hidden dimension\n",
    "        num_layers=2,          # Increased number of layers\n",
    "        bidirectional=False,    # Using a bidirectional LSTM\n",
    "        max_seq_len=60,\n",
    "        dropout=0.2764879613755455,           # Increased dropout to prevent overfitting\n",
    "        optimizer='adamw',\n",
    "        lr=5.965328183810809e-05,\n",
    "        weight_decay=4.0205830285607866e-06,     # Adjusted weight decay\n",
    "        max_epochs=200,\n",
    "        gradient_clip_val=0.31629708444325294, # Increased gradient clipping value\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9ed514",
   "metadata": {},
   "source": [
    "# Aeon Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a25af",
   "metadata": {},
   "source": [
    "##### SharePriceIncrease TimeMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35664e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'timemil'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name [BasicMotions, SharePriceIncrease]\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(10):\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        #data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/twitter\",\n",
    "        aeon_dataset='SharePriceIncrease',\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=64, # 128\n",
    "        hidden_dim=128, # never change this for timemil\n",
    "        num_layers=1,\n",
    "        max_seq_len=60,\n",
    "        dropout= 0.4873169054805524, # current best at 0.4\n",
    "        optimizer='adamw',\n",
    "        lr= 1.082096149444034e-05,# 1e-5, # current best at 1e-5\n",
    "        weight_decay= 0.0022621952972201147,# 1e-4, # current best at 1e-4\n",
    "        max_epochs=200,\n",
    "        gradient_clip_val=0.49314467483726887,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92cc33",
   "metadata": {},
   "source": [
    "##### SharePriceIncrease TodyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96ec2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_new')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TodyNet on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'todynet'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(10):\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}_check\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    # Define todynet parameters (without 'num_nodes')\n",
    "    todynet_params = {\n",
    "        'num_layers': 3,\n",
    "        'groups': 1, # 1\n",
    "        'pool_ratio': 0.23740150417551795, #0.2\n",
    "        'kern_size': [7, 5, 3], # [9, 5, 3]\n",
    "        'hidden_dim': 64, # 128\n",
    "        'out_dim': 256, # 256\n",
    "        'dropout': 0.2625681811874664,\n",
    "        'gnn_model_type': 'dyGCN2d',\n",
    "        'in_dim': 1\n",
    "        # 'in_dim' and 'seq_length' will be set in train_experiment\n",
    "    }\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        dataset_name=dataset_name,\n",
    "        model_name=model_name,\n",
    "        #data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/twitter\",\n",
    "        aeon_dataset='SharePriceIncrease',\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=32,\n",
    "        hidden_dim=256,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.13748444505513727,\n",
    "        optimizer='adamw',\n",
    "        lr=0.0002857872828206899,\n",
    "        weight_decay=1.4790851179473734e-06,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.187201940376038,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "        todynet_params=todynet_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a604bb8f",
   "metadata": {},
   "source": [
    "##### SharePriceIncrease LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c61b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run LSTMClassifier on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'lstm_classifier'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for i in range(10):\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_run_{i}_time_{current_time}_check\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        run=i,\n",
    "        model_name=model_name,\n",
    "        dataset_name=dataset_name,\n",
    "        #data_dir=f\"/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/data/twitter\",\n",
    "        aeon_dataset='SharePriceIncrease',\n",
    "        experiment_name=experiment_name,\n",
    "        #seed=seed,\n",
    "        batch_size=64,         # Increased batch size for faster training if GPU memory allows\n",
    "        hidden_dim=128,        # Increased hidden dimension\n",
    "        num_layers=1,          # Increased number of layers\n",
    "        bidirectional=False,    # Using a bidirectional LSTM\n",
    "        max_seq_len=60,\n",
    "        dropout=0.38060887237497343,           # Increased dropout to prevent overfitting\n",
    "        optimizer='adamw',\n",
    "        lr=5.659990373880629e-05,\n",
    "        weight_decay=3.356868512223481e-06,     # Adjusted weight decay\n",
    "        max_epochs=200,\n",
    "        gradient_clip_val=0.41800486528115716, # Increased gradient clipping value\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b799d88",
   "metadata": {},
   "source": [
    "## 6.1 Running Experiments with Aeon Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ba3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_assessment')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "# Run TimeMIL on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'timemil'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name [BasicMotions, SharePriceIncrease]\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # get current_time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset='SharePriceIncrease', # dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=128,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.2,\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 50},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src_interpretability_assessment')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run TodyNet on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'todynet'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    # Define todynet parameters (without 'num_nodes')\n",
    "    todynet_params = {\n",
    "        'num_layers': 3,\n",
    "        'groups': 4,\n",
    "        'pool_ratio': 0.2,\n",
    "        'kern_size': [9, 5, 3],\n",
    "        'hidden_dim': 128,\n",
    "        'out_dim': 256,\n",
    "        'dropout': 0.3,\n",
    "        'gnn_model_type': 'dyGIN2d',\n",
    "        # 'in_dim' and 'seq_length' will be set in train_experiment\n",
    "    }\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset=dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,\n",
    "        hidden_dim=128,\n",
    "        max_seq_len=60,\n",
    "        dropout=0.2,\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=0.5,\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "        todynet_params=todynet_params\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec09a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your notebook cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Add your module path for custom imports\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new')  # Adjust this if you saved your .py files elsewhere\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/work_project/modelling_new/src')  # Adjust this if you saved your .py files elsewhere\n",
    "\n",
    "\n",
    "# Run LSTMClassifier on an aeon dataset with multiple seeds\n",
    "\n",
    "from train import train_experiment\n",
    "from datetime import datetime\n",
    "\n",
    "model_name = 'lstm_classifier'\n",
    "dataset_name = 'SharePriceIncrease'  # Replace with actual dataset name\n",
    "seeds = [46]\n",
    "\n",
    "for seed in seeds:\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    experiment_name = f\"{model_name}_{dataset_name}_seed_{seed}_time_{current_time}\"\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "    train_experiment(\n",
    "        model_name=model_name,\n",
    "        aeon_dataset=dataset_name,\n",
    "        experiment_name=experiment_name,\n",
    "        seed=seed,\n",
    "        batch_size=16,         # Increased batch size for faster training if GPU memory allows\n",
    "        hidden_dim=256,        # Increased hidden dimension\n",
    "        num_layers=3,          # Increased number of layers\n",
    "        bidirectional=False,    # Using a bidirectional LSTM\n",
    "        max_seq_len=60,\n",
    "        dropout=0.3,           # Increased dropout to prevent overfitting\n",
    "        optimizer='adamw',\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,     # Adjusted weight decay\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=1.0, # Increased gradient clipping value\n",
    "        use_class_weights=True,\n",
    "        scheduler='reduce_on_plateau',\n",
    "        scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 20},\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
